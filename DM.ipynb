{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c6e0916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy Score:  0.9164370982552801\n",
      "Precision Score:  0.9314720812182741\n",
      "Recall Score:  0.851508120649652\n",
      "F1 Score:  0.8896969696969697\n",
      "\n",
      "SVM:\n",
      "Accuracy Score:  0.8953168044077136\n",
      "Precision Score:  0.9464788732394366\n",
      "Recall Score:  0.7795823665893271\n",
      "F1 Score:  0.8549618320610687\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy Score:  0.8668503213957759\n",
      "Precision Score:  0.8404761904761905\n",
      "Recall Score:  0.8190255220417634\n",
      "F1 Score:  0.8296122209165687\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import log_loss,accuracy_score,precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\sanja\\Downloads\\archive\\dataset.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Create a copy to leave the original dataset unchanged\n",
    "students_df = df.copy() \n",
    "\n",
    "# Change the Target to Numerical Value 0 or 1. Predicting whether a student will graduate or dropout\n",
    "students_df.replace(\"Dropout\",1,inplace=True) \n",
    "students_df.replace(\"Graduate\",0,inplace=True)\n",
    "#students_df.replace(\"Enrolled\",-1,inplace=True)\n",
    "students_df.drop(students_df[students_df[\"Target\"]==\"Enrolled\"].index,inplace=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "y = students_df[\"Target\"]\n",
    "y = y.astype('int')\n",
    "X = students_df.iloc[:,:-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create the models\n",
    "log_reg = LogisticRegression()\n",
    "svm = SVC()\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the models on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "log_reg_y_pred = log_reg.predict(X_test)\n",
    "svm_y_pred = svm.predict(X_test)\n",
    "dtc_y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_y_pred)\n",
    "log_reg_precision = precision_score(y_test, log_reg_y_pred)\n",
    "log_reg_recall = recall_score(y_test, log_reg_y_pred)\n",
    "log_reg_f1 = f1_score(y_test, log_reg_y_pred)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_precision = precision_score(y_test, svm_y_pred)\n",
    "svm_recall = recall_score(y_test, svm_y_pred)\n",
    "svm_f1 = f1_score(y_test, svm_y_pred)\n",
    "\n",
    "dtc_accuracy = accuracy_score(y_test, dtc_y_pred)\n",
    "dtc_precision = precision_score(y_test, dtc_y_pred)\n",
    "dtc_recall = recall_score(y_test, dtc_y_pred)\n",
    "dtc_f1 = f1_score(y_test, dtc_y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy Score: \", log_reg_accuracy)\n",
    "print(\"Precision Score: \", log_reg_precision)\n",
    "print(\"Recall Score: \", log_reg_recall)\n",
    "print(\"F1 Score: \", log_reg_f1)\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "print(\"Accuracy Score: \", svm_accuracy)\n",
    "print(\"Precision Score: \", svm_precision)\n",
    "print(\"Recall Score: \", svm_recall)\n",
    "print(\"F1 Score: \", svm_f1)\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(\"Accuracy Score: \", dtc_accuracy)\n",
    "print(\"Precision Score: \", dtc_precision)\n",
    "print(\"Recall Score: \", dtc_recall)\n",
    "print(\"F1 Score: \", dtc_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3252713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM with GridSearch:\n",
      "Accuracy Score:  0.9173553719008265\n",
      "Precision Score:  0.9428571428571428\n",
      "Recall Score:  0.8422273781902552\n",
      "F1 Score:  0.8897058823529411\n",
      "Best Score:  0.9082963352567177\n",
      "Best Parameters:  {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Decision Tree with GridSearch:\n",
      "Accuracy Score:  0.8888888888888888\n",
      "Precision Score:  0.9122340425531915\n",
      "Recall Score:  0.7958236658932715\n",
      "F1 Score:  0.8500619578686494\n",
      "Best Score:  0.8843014711569698\n",
      "Best Parameters:  {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "\n",
      "Logistic Regression with GridSearch:\n",
      "Accuracy Score:  0.9164370982552801\n",
      "Precision Score:  0.9314720812182741\n",
      "Recall Score:  0.851508120649652\n",
      "F1 Score:  0.8896969696969697\n",
      "Best Score:  0.9118396423433317\n",
      "Best Parameters:  {'C': 1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.90711678        nan 0.91183964        nan 0.90947821]\n",
      "  warnings.warn(\n",
      "C:\\Users\\sanja\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for SVM\n",
    "parameter_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "svm_grid = GridSearchCV(SVC(), parameter_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using the best model\n",
    "svm_y_pred = svm_grid.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "svm_grid_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_grid_precision = precision_score(y_test, svm_y_pred)\n",
    "svm_grid_recall = recall_score(y_test, svm_y_pred)\n",
    "svm_grid_f1 = f1_score(y_test, svm_y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nSVM with GridSearch:\")\n",
    "print(\"Accuracy Score: \", svm_grid_accuracy)\n",
    "print(\"Precision Score: \", svm_grid_precision)\n",
    "print(\"Recall Score: \", svm_grid_recall)\n",
    "print(\"F1 Score: \", svm_grid_f1)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Score: \", svm_grid.best_score_)\n",
    "print(\"Best Parameters: \", svm_grid.best_params_)\n",
    "\n",
    "# Define the parameter grid for Decision Tree\n",
    "parameter_grid = {'max_depth': [1, 5, 10, 15], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "dtree_grid = GridSearchCV(DecisionTreeClassifier(), parameter_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "dtree_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using the best model\n",
    "dtree_y_pred = dtree_grid.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "dtree_grid_accuracy = accuracy_score(y_test, dtree_y_pred)\n",
    "dtree_grid_precision = precision_score(y_test, dtree_y_pred)\n",
    "dtree_grid_recall = recall_score(y_test, dtree_y_pred)\n",
    "dtree_grid_f1 = f1_score(y_test, dtree_y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nDecision Tree with GridSearch:\")\n",
    "print(\"Accuracy Score: \", dtree_grid_accuracy)\n",
    "print(\"Precision Score: \", dtree_grid_precision)\n",
    "print(\"Recall Score: \", dtree_grid_recall)\n",
    "print(\"F1 Score: \", dtree_grid_f1)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Score: \", dtree_grid.best_score_)\n",
    "print(\"Best Parameters: \", dtree_grid.best_params_)\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "parameter_grid = {'C': [0.1, 1, 5], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "lr_grid = GridSearchCV(LogisticRegression(), parameter_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using the best model\n",
    "lr_y_pred = lr_grid.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lr_accuracy = accuracy_score(y_test, lr_y_pred)\n",
    "lr_precision = precision_score(y_test, lr_y_pred)\n",
    "lr_recall = recall_score(y_test, lr_y_pred)\n",
    "lr_f1 = f1_score(y_test, lr_y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nLogistic Regression with GridSearch:\")\n",
    "print(\"Accuracy Score: \", lr_accuracy)\n",
    "print(\"Precision Score: \", lr_precision)\n",
    "print(\"Recall Score: \", lr_recall)\n",
    "print(\"F1 Score: \", lr_f1)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Score: \", lr_grid.best_score_)\n",
    "print(\"Best Parameters: \", lr_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f59db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(215,210))\n",
    "plot_tree(tree, filled=True, feature_names=X.columns, class_names=['Graduate', 'Dropout'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ac061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to plot\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "# Define the scores for each model and metric\n",
    "log_reg_scores = [accuracy_score(y_test, log_reg_y_pred), \n",
    "                  precision_score(y_test, log_reg_y_pred), \n",
    "                  recall_score(y_test, log_reg_y_pred), \n",
    "                  f1_score(y_test, log_reg_y_pred)]\n",
    "\n",
    "svm_scores = [accuracy_score(y_test, svm_y_pred), \n",
    "              precision_score(y_test, svm_y_pred), \n",
    "              recall_score(y_test, svm_y_pred), \n",
    "              f1_score(y_test, svm_y_pred)]\n",
    "\n",
    "tree_scores = [accuracy_score(y_test, dtc_y_pred), \n",
    "               precision_score(y_test, dtc_y_pred), \n",
    "               recall_score(y_test, dtc_y_pred), \n",
    "               f1_score(y_test, dtc_y_pred)]\n",
    "\n",
    "# Plot the bar chart\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "ax.bar(x - width, log_reg_scores, width, label='Logistic Regression')\n",
    "ax.bar(x, svm_scores, width, label='SVM')\n",
    "ax.bar(x + width, tree_scores, width, label='Decision Tree')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparison of Model Performance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot histogram of features\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.histplot(data=df, x=\"Age at enrollment\", hue=\"Target\", kde=True)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84591c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\n",
    "pca_df['Target'] = y_train.values\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=pca_df['Target'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d297910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the principal components and their weights\n",
    "pcs = pca.components_\n",
    "weights = np.abs(pcs)\n",
    "\n",
    "# Find the component with the highest contribution\n",
    "highest_contrib_comp = np.argmax(weights.sum(axis=1))\n",
    "\n",
    "# Print the column names and weights for the highest contributing component\n",
    "\n",
    "print(\"Weights: \", pcs[highest_contrib_comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding highest contributor to the PCA\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train_scaled)\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=X_train.columns)\n",
    "contributions = pd.DataFrame(np.abs(loadings) * pca.explained_variance_ratio_, columns=['PC1', 'PC2'], index=X_train.columns)\n",
    "total_contributions = contributions.sum(axis=1)\n",
    "highest_contributor = total_contributions.idxmax()\n",
    "print(f'The column with the highest contribution to PCA is {highest_contributor}, with a total contribution of {total_contributions[highest_contributor]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59924d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Regular SVM', 'GridSearchCV SVM']\n",
    "accuracy_scores = [svm_accuracy, svm_grid_accuracy]\n",
    "plt.bar(labels, accuracy_scores, width=0.4, color='green')\n",
    "for i, v in enumerate(accuracy_scores):\n",
    "    plt.text(i - 0.12, v + 0.02, str(round(v, 3)), fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Accuracy Scores Comparison')\n",
    "plt.show()\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc78e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
